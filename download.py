import torch
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from transformers import BitsAndBytesConfig
import time


MODEL_ID = "Qwen/Qwen2.5-VL-7B-Instruct"

# 4-–±–∏—Ç–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

print(f"üöÄ {MODEL_ID} –≥–æ—Ç–æ–≤ –∫ –∑–∞–≥—Ä—É–∑–∫–µ –∏ 4-bit –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—é!")
print("--- –í –ø–µ—Ä–≤—ã–π —Ä–∞–∑ —ç—Ç–æ —Å–∫–∞—á–∞–µ—Ç ~15GB –¥–∞–Ω–Ω—ã—Ö! ---")

# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ (–û–Ω–∞ —Å–∞–º–∞ —Å–∫–∞—á–∞–µ—Ç —Ñ–∞–π–ª—ã —Å Hugging Face)
try:
    start_time = time.time()
    

    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        MODEL_ID,
        device_map="auto",          # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–≤–æ—é RTX 3050
        quantization_config=bnb_config, 
        trust_remote_code=True,
        # –≠—Ç–æ –Ω–µ –≤—Å–µ–≥–¥–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å, –Ω–æ –∑–∞–≥—Ä—É–∑–∫–∞ –±—É–¥–µ—Ç –∏–¥—Ç–∏!
        # –°–º–æ—Ç—Ä–∏ –Ω–∞ –∫–æ–Ω—Å–æ–ª—å, —Ç–∞–º –±—É–¥—É—Ç –ø–æ–ª–æ—Å–∫–∏ Downloading: xx%
    )
    
    end_time = time.time()
    
except Exception as e:
    print(f"‚ùå –ê–•–¢–£–ù–ì! –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
    exit()

# 2. –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)

print("---")
print(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, —Å–∂–∞—Ç–∞ –≤ 4-bit –∏ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
print(f"‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏: {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥ (–ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ –±—É–¥–µ—Ç –¥–æ–ª–≥–∏–º –∏–∑-–∑–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è!)")
print("---")

#!–¢–ï–°–¢! (–ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å—ë —Ä–∞–±–æ—Ç–∞–µ—Ç)
text_input = "–û–±—ä—è—Å–Ω–∏ —á—Ç–æ —Ç–∞–∫–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞—è –º–∞—Ç–µ—Ä–∏—è" # –ü–†–û–ú–ü–¢
print(f"‚ùì –¢–≤–æ–π –≤–æ–ø—Ä–æ—Å: {text_input}")

messages = [
    {"role": "user", "content": [{"type": "text", "text": text_input}]}
]

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–≤–æ–¥–∞
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = processor(text=[text], padding=True, return_tensors="pt")
inputs = inputs.to("cuda")

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
print("ü§ñ –î—É–º–∞—é... (–°–º–æ—Ç—Ä–∏ –∑–∞–≥—Ä—É–∑–∫—É VRAM –≤ –î–∏—Å–ø–µ—Ç—á–µ—Ä–µ –∑–∞–¥–∞—á!)")
generated_ids = model.generate(**inputs, max_new_tokens=412)
generated_ids_trimmed = [
    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)

print(f"\nüó£Ô∏è –û–¢–í–ï–¢ –ò–ò: {output_text[0]}")